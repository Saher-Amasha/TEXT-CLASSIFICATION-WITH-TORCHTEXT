{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saher-Amasha/TEXT-CLASSIFICATION-WITH-TORCHTEXT/blob/main/saher_amasha_207607136_safa_amasha_211941943_hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRhKpE6o2Xga",
        "outputId": "40e619b0-c8ca-430f-f0b9-208fca73f3ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.11.0+cu113)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchdata) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#get the ``AG_NEWS`` dataset iterators"
      ],
      "metadata": {
        "id": "sTpL2WaM2_YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "train_iter = iter(AG_NEWS(split='train'))"
      ],
      "metadata": {
        "id": "822tZqpU22Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare data processing pipelines"
      ],
      "metadata": {
        "id": "6qcp2D5l3Fmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "Bh_Ct8kD24y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the text processing pipeline with the tokenizer and vocabulary"
      ],
      "metadata": {
        "id": "p_Z4ghUZ3V-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "metadata": {
        "id": "YaV9cFCi3SSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate data batch and iterator"
      ],
      "metadata": {
        "id": "ezhI39ww3cVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "ptHPDS7D3gzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model"
      ],
      "metadata": {
        "id": "zjkuBxpE3h5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "metadata": {
        "id": "XBlcmfQq3mSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initiate an instance"
      ],
      "metadata": {
        "id": "hS7xUQ313r4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "metadata": {
        "id": "sF1ZKuiL3slt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define functions to train the model and evaluate results"
      ],
      "metadata": {
        "id": "OGCMUZsb3u4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text, offsets)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predicted_label = model(text, offsets)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "metadata": {
        "id": "t4xFYU633y70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the dataset and run the model"
      ],
      "metadata": {
        "id": "5fKXCNLz39FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtTacsBC4EMe",
        "outputId": "dc8a58b9-876a-4e72-e3a1-386846dda910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 1782 batches | accuracy    0.682\n",
            "| epoch   1 |  1000/ 1782 batches | accuracy    0.854\n",
            "| epoch   1 |  1500/ 1782 batches | accuracy    0.880\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time:  7.64s | valid accuracy    0.881 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 1782 batches | accuracy    0.900\n",
            "| epoch   2 |  1000/ 1782 batches | accuracy    0.900\n",
            "| epoch   2 |  1500/ 1782 batches | accuracy    0.903\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time:  7.48s | valid accuracy    0.892 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 1782 batches | accuracy    0.916\n",
            "| epoch   3 |  1000/ 1782 batches | accuracy    0.913\n",
            "| epoch   3 |  1500/ 1782 batches | accuracy    0.915\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  7.27s | valid accuracy    0.900 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 1782 batches | accuracy    0.925\n",
            "| epoch   4 |  1000/ 1782 batches | accuracy    0.921\n",
            "| epoch   4 |  1500/ 1782 batches | accuracy    0.922\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time:  7.37s | valid accuracy    0.901 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 1782 batches | accuracy    0.929\n",
            "| epoch   5 |  1000/ 1782 batches | accuracy    0.930\n",
            "| epoch   5 |  1500/ 1782 batches | accuracy    0.930\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time:  7.40s | valid accuracy    0.903 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 1782 batches | accuracy    0.938\n",
            "| epoch   6 |  1000/ 1782 batches | accuracy    0.936\n",
            "| epoch   6 |  1500/ 1782 batches | accuracy    0.931\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time:  7.73s | valid accuracy    0.895 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 1782 batches | accuracy    0.945\n",
            "| epoch   7 |  1000/ 1782 batches | accuracy    0.944\n",
            "| epoch   7 |  1500/ 1782 batches | accuracy    0.950\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time:  7.29s | valid accuracy    0.906 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 1782 batches | accuracy    0.951\n",
            "| epoch   8 |  1000/ 1782 batches | accuracy    0.948\n",
            "| epoch   8 |  1500/ 1782 batches | accuracy    0.948\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time:  7.68s | valid accuracy    0.906 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 1782 batches | accuracy    0.950\n",
            "| epoch   9 |  1000/ 1782 batches | accuracy    0.951\n",
            "| epoch   9 |  1500/ 1782 batches | accuracy    0.948\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time:  7.43s | valid accuracy    0.907 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 1782 batches | accuracy    0.950\n",
            "| epoch  10 |  1000/ 1782 batches | accuracy    0.950\n",
            "| epoch  10 |  1500/ 1782 batches | accuracy    0.950\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time:  7.79s | valid accuracy    0.907 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the results of the test dataset"
      ],
      "metadata": {
        "id": "2KvQaU0p4vKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z945SJ_PbCQK",
        "outputId": "f89af3c3-d17c-4389-9cea-7fb3c1812d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test on a random news"
      ],
      "metadata": {
        "id": "GHc00hhl4xyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNUp_3-I2bEK",
        "outputId": "85aca67f-9a52-4a7c-d7ee-68a34ce7cb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a Sports news\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex2_test=\"The restrictions against Huawei and ZTE were announced by the country's industry minister on Thursday\\\n",
        "Francois-Philippe Champagne says the move will improve Canada's mobile internet services and \\\"protect the safety and security of Canadians\\\".\\\n",
        "But Huawei Canada said it was \\\"disappointed\\\" by the decision, which it said was \\\"political\\\".\\\n",
        "\\\"This is an unfortunate political decision that has nothing to do with cyber security or any of the technologies in question,\\\" a statement said.\\\n",
        "Several nations - including the UK, US, Australia and New Zealand - have already put restrictions on the firms.\\\n",
        "The four countries, along with Canada, make up an intelligence-sharing arrangement named \\\"Five Eyes\\\". It evolved during the Cold War as a mechanism for monitoring the Soviet Union and sharing classified information.\"\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex2_test, text_pipeline)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dULvoLRZ4mTD",
        "outputId": "ebb5461d-f749-4654-dc5c-d9fce6733657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a Sci/Tec news\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex3_test=\"ustralian voters have delivered a sharp rebuke to the center-right government, ending nine years of conservative rule, in favor of the center-left opposition that promised stronger action on climate change.\\\n",
        "Australian Labor Party leader Anthony Albanese appeared certain to form a minority government, though it was unclear as counting continued if the party would have enough seats for a majority, according to projections from three news networks.\\\n",
        "Parties need a majority of 76 seats to form a majority government. Labor is currently sitting on around 70, according to the Australian Electoral Commission.\"\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex3_test, text_pipeline)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdWIvKDI5zAJ",
        "outputId": "d1219fc3-b11d-4d0d-db71-53217281b76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a World news\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hJbLT7R88RPe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "saher_amasha_207607136_safa_amasha_211941943_hw3",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}